{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "langid.classify(\"This is a test\")\n",
    "# ('en', -54.41310358047485)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langid.classify(\"danke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(\"hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"hello how are you doing my friend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(langid.classify (d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = langid.classify (d)\n",
    "print (w[0])\n",
    "if w[0] == \"en\":\n",
    "    print (\"*******************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (langid.classify (d))[0] == \"en\":\n",
    "    print (\"*******************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'Heloo'.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"common_words.txt\")\n",
    "seek = \"you\"\n",
    "if seek in file.read():\n",
    "    print(\"found\")\n",
    "else:\n",
    "    print(\"not found\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'f' in english_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "if wordnet.synsets(\"hello\"):\n",
    "    print (\"found\")\n",
    "else:\n",
    "    print (\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "sent = \"Io andiamo to the beach with my amico.\"\n",
    "\" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
    "         if w.lower() in words or not w.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"fa\"\n",
    "if len(word) >= 2:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 >= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [\"Hello\", \"world\"]\n",
    "','.join(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['this','is','a','sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kt = \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(kt))\n",
    "type(repr(kt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['g','e','e','k', 's']  \n",
    "ss=\"\".join(list1)\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ll\",ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \",\".join(\"World word\")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install clean-text[gpl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(\"+35 | | `texts_passing`    |       +1 | | `texts_total`      |       +1 | ## Wor  ## Global | Changed            |   Status | |:-------------------|---------:| | `metadata_passing` |       +2 | | `metadata_total`   |       +2 | | `nodes_count`      |      +44 | | `texts_passing`    |       +2 | | `texts_total`      |       +2 | ## Wor  ## Global | Changed            |   Status | |:-------------------|---------:| | `coverage`         |        4 | | `metadata_passing` |       32 | | `metadata_total`   |       31 | | `nodes_count`      |    11186 | | `texts_passing`    |       62 | ## Wor  ## Global | Changed            |   Status | |:-------------------|---------:| | `coverage`         |     4.05 | | `metadata_passing` |\",\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=True,                # replace all digits with a special token\n",
    "    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "    no_punct=True,                 # fully remove punctuation\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_phone_number=\"<PHONE>\",\n",
    "    replace_with_number=\"<NUMBER>\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"<CUR>\",\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow compatibility between Python 2.7 and 3.5\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open \n",
    "#####\n",
    "\n",
    "import glob # Allows batch input file fetching\n",
    "import os   # Allows convenient file handling\n",
    "import time # Calculates script runtime\n",
    "import text_clean_utils # Collection of helper functions in utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install text_clean_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    " \n",
    "\"\"\"This script automates text cleaning over one or more text files.\n",
    "Input should consist one or more messages separated by '|' dividers\n",
    "   \n",
    "Run the following at the Terminal prompt to get started for Python 2.7:\n",
    "    \n",
    "    $ python2 text_clean.py --input example_texts\n",
    "    \n",
    "       or (for Python 3.x)\n",
    "       \n",
    "    $ python3 text_clean.py --input example_texts\n",
    "    \n",
    "The core spell correcting function requires the Python nltk package.\n",
    "In order to install this in your current environment, input the following at\n",
    "your Terminal prompt:\n",
    "    \n",
    "    $ pip install nltk\n",
    "-------------------------------\n",
    "Currently the following cleaning functions are supported for\n",
    "each message in the text file:\n",
    "       \n",
    "1.) Repeating tokens are removed; first instance is kept:\n",
    "    >>> \"I am John John John\" becomes \"I am John\"\n",
    "    \n",
    "2.) Mixed type tokens are removed, e.g. \"John23\", \"$Max$\", however some special\n",
    "    cases are kept: \n",
    "    >>> Dollars ($5, $5,000)\n",
    "    >>> Percentages (2%, 2,000%)\n",
    "    >>> HH:MM Times (4:00, 17:00)\n",
    "    >>> Ordinals (5th, 22nd, 33rd, 71st)\n",
    "    >>> Punctuation at end of token (Hello!, Yes?, Jacks', 101,)\n",
    "    >>> Apostrophe tokens (Don't, Didn't, Jack's)\n",
    "      \n",
    "3.) Long tokens are removed (character string length greater than 13):\n",
    "    >>> \"1234567890123455\", \"nowthishereisareallylongword\"\n",
    "    \n",
    "4.) Tokens with three or more repeating characters are removed:\n",
    "    >>> \"Rogggger\", \"1000000\" \n",
    "    \n",
    "5.) All non-punctuation symbols are removed (@, ^, #, etc.), however math \n",
    "    expressions are kept:\n",
    "    >>> \"2 + 2\", \"5 * 5\", \"7 - 7 = 0\")\n",
    "    \n",
    "6.) Repeating quad-groups, tri-groups, and bi-groups are removed; first\n",
    "   instance is kept:\n",
    "    >>> \"I am watching I am watching I am watching I am watching\" becomes\n",
    "    >>>   \"I am watching\"\n",
    "7.) Gibberish tokens are removed (this involves subjective discretion):\n",
    "    >>> \"alskdjfaasdlfjkasd\", \"s\", \"iaaiuuuuwu\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Allows parsing of user-specified options at script call\n",
    "from optparse import OptionParser \n",
    "\n",
    "# Initialize parser, then add arguments. Input directory is required. \n",
    "PARSER = OptionParser()\n",
    "\n",
    "PARSER.add_option('-i', '--input', dest='input_directory',\n",
    "                  help='input directory with files that need text cleaning')\n",
    "PARSER.add_option('-v', '--verbose', dest='verbose', default=\"True\",\n",
    "                  help='toggle console output')\n",
    "\n",
    "OPTIONS, _ = PARSER.parse_args() # Convert options into usable strings\n",
    "\n",
    "# Load user options into global constants for use in the script\n",
    "INPUT_DIR = OPTIONS.input_directory # Directory containing input files (required)\n",
    "VERBOSE = OPTIONS.verbose # Display console output (optional)\n",
    "\n",
    "# Ensure input directory exists, otherwise the script exits\n",
    "assert os.path.isdir(INPUT_DIR), \\\n",
    "        \"The input directory doesn't exist in the working directory\"\n",
    "\n",
    "# Default name for output directory, change if needed\n",
    "OUTPUT_DIR = 'cleaned_output'  \n",
    "# Check if output directory exists, otherwise create one\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "    \n",
    "##### Main Function #####\n",
    "def clean_text(input_dir, verbose='True'):\n",
    "\n",
    "    \"\"\"Takes an input directory and processes all .txt files within, cleaning\n",
    "    all contents within each one. Outputs to a separate directory cleaned \n",
    "    messages in files with the --CLEANED flag\n",
    "    \n",
    "    `input_dir` = name of directory that contains input .txt files\n",
    "    `verbose` = toggles console output (default True)   \n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time() # Start runtime\n",
    "    \n",
    "    # Set verbose flag for use throughout\n",
    "    verbose = text_clean_utils.str_to_bool(verbose)\n",
    "\n",
    "    # Initialize generator that fetches message files one by one\n",
    "    messages_fetcher = glob.iglob('{}/*.txt'.format(input_dir))\n",
    "    \n",
    "    # Begin script\n",
    "    if verbose:\n",
    "        print(\"\\n------------------------------\\n\"\n",
    "              \"Text cleaning starting\\n\"\n",
    "              \"------------------------------\\n\")\n",
    "    \n",
    "    attempted_files = 0 # Track how many files attempted conversion\n",
    "    completed_files = 0 # Track how many files converted successfully\n",
    "    \n",
    "    # Main loop that batch processes input files \n",
    "    while messages_fetcher:\n",
    "        try:\n",
    "            current_messages_file = next(messages_fetcher) # Fetch next messages file\n",
    "        except StopIteration:\n",
    "            break # Execute when there are no more files to fetch\n",
    "\n",
    "        # Create a path for the output file in the output directory\n",
    "        # Flag each output file with '--CLEANED', indicating that any\n",
    "            # messy text has been found and cleaned\n",
    "        new_messages_file = os.path.join(OUTPUT_DIR, '{0}--CLEANED{1}'.format(\n",
    "            *os.path.splitext(os.path.basename(current_messages_file))))\n",
    "\n",
    "        attempted_files += 1 # Increment prior to processing\n",
    "\n",
    "        # Clean all messages in current_messages_file\n",
    "        cleaned_messages = text_clean_utils.clean_text(current_messages_file)\n",
    "       \n",
    "        # Write new messages to file in output directory at the path\n",
    "            # specified in current_output variable\n",
    "        with open(new_messages_file, 'w', encoding='utf-8') as file_handle:\n",
    "            file_handle.write(cleaned_messages)\n",
    "        # Display where current output file is located on the local file system\n",
    "        if verbose:\n",
    "            print(\"*\".rjust(5), \"\\t\", current_messages_file, \">>>\", new_messages_file)\n",
    "\n",
    "        # Increment upon successful processing of current input file\n",
    "        completed_files += 1\n",
    "\n",
    "    # Main loop ends\n",
    "    end_time = time.time() # Runtime ends\n",
    "\n",
    "    # Output Report\n",
    "    if verbose:\n",
    "        print(\"\\n--------------------------------------------\\n\"\n",
    "              \"Text cleaning complete\\n\"\n",
    "              \"{0} of {1} files cleaned in {2} minutes\\n\"\n",
    "              \"--------------------------------------------\\n\".format(\n",
    "                  completed_files, attempted_files, round((\n",
    "                      end_time - start_time) / 60, 3)))\n",
    "        \n",
    "    # Exit\n",
    "    return\n",
    "\n",
    "# Standard code that allows the file to be run as a script from the terminal   \n",
    "if __name__ == '__main__':\n",
    "    clean_text(INPUT_DIR, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=['123','cn',]\n",
    "if numpy.array(list1).size:\n",
    "    print(\"empty\")\n",
    "else:\n",
    "    print(\"not empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-58727ec1a867>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'123'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"not empty\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"empty\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "list1=['123','cn',]\n",
    "if list1[2] in list1:\n",
    "    print(\"not empty\")\n",
    "else:\n",
    "    print(\"empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=['123','cn','un d rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list1) != 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
